{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.tensorrt as trt\n",
    "import tensorflow.keras as tfk\n",
    "import random\n",
    "import os\n",
    "import pathlib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.compat.v1.graph_util import convert_variables_to_constants\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.python.tools import freeze_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "keras_model_pfad = \"keras_model_0.h5\"\n",
    "tf_model_pfad = os.path.join(base_path,'../Models/Tensor-Model/Tensor_')\n",
    "fr_graph_tf_model = tf_model_pfad + \"frozen_model_tf.pb\"\n",
    "fr_graph_trt_model =os.path.join(base_path,'../Models/RT-Model/frozen_model_trt.pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras zu Tensorflow-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"\n",
    "diese Funktion wird ein Keras-Model in ein Tensor-Model\n",
    "Umwandeln\n",
    "\"\"\"\n",
    "\"\"\" da die Batchnormalisation-Parameter im Keras\n",
    "als trainable parameter gespeichert sind, mussen wird diese als\n",
    "non_trainable zurücksetzen\n",
    "\"\"\"\n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "\n",
    "print(\"Keras-Model wird hochgeladen!\")\n",
    "try:\n",
    "    model = load_model(keras_model_pfad)\n",
    "except FileNotFoundError:\n",
    "    print(\"Kein .h5-Datei wurde gefunden!\")\n",
    "    exit()\n",
    "# get input und output des models for tensorflow improvement\n",
    "input_model = model.inputs[0]\n",
    "print(input_model)\n",
    "output_model = model.outputs[0]\n",
    "\n",
    "print(\"Umwandlung vom Keras zu Tensor-Model..\")\n",
    "# model.summary()\n",
    "# [print(node.op.name) for node in model.outputs]\n",
    "saver = tf.train.Saver()\n",
    "# keras session wird als tf- Session gelesen!\n",
    "sess = tf.keras.backend.get_session()\n",
    "# die Graph der Tf-Session wird gespeichert\n",
    "save_path = saver.save(sess, tf_model_pfad)\n",
    "tf.keras.backend.clear_session()\n",
    "print(\"Umgewandelt und gespeichert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umwandlung vom Tensorflow-Model zu TensorRT-Frozen-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es wird im folden Skript die Optimierung vom Tensorflow-Model durch Tensorflow-RT module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " das aus Keras-Model Generierte Tensort-Model(oder ein normales\n",
    " Tensor-Model) wird hochgeladen\n",
    " und in frozen Graph umgewandelt\n",
    " input : tensor-Model\n",
    " ouput : frozen graph .pd\n",
    "\"\"\"\n",
    "# set the memory fraction eg. 0.2 meaning tf use 20% of gpu and\n",
    "# the will bei Trt\n",
    "gpu_option = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_option)) as sess:\n",
    "    # import of meta graph fo tensorflow model\n",
    "    path_meta_file = tf_model_pfad + \".meta\"\n",
    "    saver = tf.train.import_meta_graph(path_meta_file)\n",
    "    # print(graph)\n",
    "    # exit()\n",
    "    # restore the weights to the meta graph\n",
    "    saver.restore(sess, tf_model_pfad)\n",
    "\n",
    "    # specify the tensor output of the model\n",
    "    output_models = [n.name for\n",
    "                     n in tf.get_default_graph().as_graph_def().node\n",
    "                     if \"dense_1/Softmax\" in n.name]\n",
    "    print(output_models)\n",
    "    # print(output_models)\n",
    "    # exit()\n",
    "    # convert to frozen model\n",
    "    print(\"Frozen model ergestellt!\")\n",
    "    frozen_graph = convert_variables_to_constants(sess,\n",
    "                                                  tf.get_default_graph().as_graph_def(),\n",
    "                                                  output_node_names=[\"dense_1/Softmax\"])\n",
    "\n",
    "    # save the frozen graph\n",
    "    print(\"speichert!\")\n",
    "    with gfile.FastGFile(fr_graph_tf_model, \"wb\") as f:\n",
    "        f.write(frozen_graph.SerializeToString())\n",
    "    print(\"Frozen model wurde gespeichert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow-Frozen zu TensorRT-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellung von TRT-Frozen-Model\n",
    "## read tensorflozen-Graph\n",
    "with tf.gfile.GFile(fr_graph_tf_model, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "trt_graph = trt.create_inference_graph(\n",
    "    input_graph_def=graph_def,\n",
    "    outputs=[\"dense_1/Softmax\"],\n",
    "    max_batch_size=1,\n",
    "    max_workspace_size_bytes=1 << 32,\n",
    "    precision_mode=\"FP16\")\n",
    "\n",
    "# save Frozen-Graph von TensorRT\n",
    "if not pathlib.Path(fr_graph_trt_model).exists():\n",
    "    os.mknod(fr_graph_trt_model)\n",
    "with gfile.FastGFile(fr_graph_trt_model, \"wb\") as f:\n",
    "    f.write(trt_graph.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Improve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE = 48\n",
    "input_node = \"conv2d_input:0\"\n",
    "output_node = \"dense_1/Softmax:0\"\n",
    "pfad_beschreibung = os.path.join(base_path,\"../Daten/utils/Text_Beschreibung.csv\")\n",
    "classe_label = pd.read_csv(pfad_beschreibung)\n",
    "classe_label = classe_label.set_index('ClassId').T.to_dict()\n",
    "\n",
    "# Bilder werden einglesen im RGB-Format\n",
    "img1 = cv2.imread(\"00000_00010.ppm\", cv2.IMREAD_UNCHANGED)\n",
    "print(\"gelesen wird:\",classe_label[0]['SignName'])\n",
    "img2 = cv2.imread(\"00000_00006.ppm\", cv2.IMREAD_UNCHANGED)\n",
    "print(\"gelesen wird:\",classe_label[3]['SignName'])\n",
    "\n",
    "image1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "image2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#image preporcessing\n",
    "image1 = cv2.resize(image1, (IMG_SIZE,IMG_SIZE), interpolation = cv2.INTER_AREA) \n",
    "image2 = cv2.resize(image2, (IMG_SIZE,IMG_SIZE), interpolation = cv2.INTER_AREA) \n",
    "\n",
    "images = [image1, image2]\n",
    "labels_classes = [0, 3]\n",
    "\n",
    "# show image\n",
    "fig=plt.figure(figsize=(4, 4))\n",
    "columns = 2\n",
    "rows = 1\n",
    "count = 1\n",
    "for real_image in zip(images, labels_classes):\n",
    "    fig.add_subplot(rows, columns,count)\n",
    "    count +=1\n",
    "    real_label = classe_label[real_image[1]]['SignName']\n",
    "    real_label = real_label.replace(\" \", \"\\n\")\n",
    "    plt.title(real_label)\n",
    "    plt.imshow(real_image[0])       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_model(path_frozen_graph, input_node, output_node, input_img):\n",
    "    \"\"\"\n",
    "    Diese funktion wird dafür helfen, irgenwelches Model(Tensorflow/TensorRt)\n",
    "    anzuwenden\n",
    "    \"\"\"\n",
    "    gpu_option = tf.GPUOptions(per_process_gpu_memory_fraction=0.50)\n",
    "    frozen_graph = tf.Graph()\n",
    "    with frozen_graph.as_default():\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_option)) as sess:\n",
    "            print(\"der Graph wird gelesen!\")\n",
    "            with tf.gfile.GFile(fr_graph_tf_model, \"rb\") as f:\n",
    "                frozen_graph = tf.GraphDef()\n",
    "                frozen_graph.ParseFromString(f.read())\n",
    "            print(\"der wurde gelesen!\")\n",
    "\n",
    "            # Auswahl von input und output\n",
    "            tf.import_graph_def(frozen_graph, name='')\n",
    "            input_tensor = sess.graph.get_tensor_by_name(input_node)\n",
    "            output_tensor = sess.graph.get_tensor_by_name(output_node)\n",
    "\n",
    "            total_time = 0\n",
    "            n_time_inference = 50\n",
    "            input_img = input_img.reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "            out_pred = sess.run(output_tensor,\n",
    "                                feed_dict={input_tensor: input_img})\n",
    "\n",
    "            for i in range(n_time_inference):\n",
    "                t1 = time.time()\n",
    "                out_pred = sess.run(output_tensor,\n",
    "                                    feed_dict={input_tensor: input_img})\n",
    "                t2 = time.time()\n",
    "                delta_time = t2 - t1\n",
    "                total_time += delta_time\n",
    "                print(\"gebrauchte Zeit - \" + str(i) + \": \", delta_time)\n",
    "            print(\"mittelre Zeit: {}\".format(total_time/n_time_inference))\n",
    "            mittelre_zeit = total_time/n_time_inference\n",
    "    return out_pred, mittelre_zeit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen -Graph Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_classes = []\n",
    "sum_zeit=0\n",
    "for image in images:\n",
    "    prediction, zeit = perform_model(fr_graph_tf_model,\n",
    "                                     input_node,\n",
    "                                     output_node,\n",
    "                                     image)\n",
    "    prediction_classes.append(prediction)\n",
    "    sum_zeit += zeit\n",
    "\n",
    "# show image\n",
    "fig=plt.figure(figsize=(4, 4))\n",
    "columns = 2\n",
    "rows = 2\n",
    "count = 1\n",
    "for image, label_class,predicted_class in zip(images, labels_classes, prediction_classes):\n",
    "    fig.add_subplot(rows, columns,count)\n",
    "    count +=1\n",
    "    real_label = classe_label[label_class]['SignName']\n",
    "    real_label = real_label.replace(\" \", \"\\n\")\n",
    "    predicted_label = classe_label[np.argmax(predicted_class[0])]['SignName']\n",
    "    predicted_label = predicted_label.replace(\" \", \"\\n\")\n",
    "    plt.title(\"Das Model hat predictd: {}\\n und Rictig ist: {}\\n\".format(predicted_label, real_label))\n",
    "    plt.imshow(image)\n",
    "label = 'Ergebnisse von Tensorflow in {} secunde Zeit '.format(sum_zeit/len(images))\n",
    "fig.suptitle(label, fontsize=16, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorRT - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_classes = []\n",
    "sum_zeit=0\n",
    "for image in images:\n",
    "    prediction, zeit = perform_model(fr_graph_trt_model,\n",
    "                                     input_node,\n",
    "                                     output_node,\n",
    "                                     image)\n",
    "    prediction_classes.append(prediction)\n",
    "    sum_zeit += zeit\n",
    "\n",
    "# show image\n",
    "fig=plt.figure(figsize=(4, 4))\n",
    "columns = 2\n",
    "rows = 2\n",
    "count = 1\n",
    "for image, label_class,predicted_class in zip(images, labels_classes, prediction_classes):\n",
    "    fig.add_subplot(rows, columns,count)\n",
    "    count +=1\n",
    "    real_label = classe_label[label_class]['SignName']\n",
    "    real_label = real_label.replace(\" \", \"\\n\")\n",
    "    predicted_label = classe_label[np.argmax(predicted_class[0])]['SignName']\n",
    "    predicted_label = predicted_label.replace(\" \", \"\\n\")\n",
    "    plt.title(\"Das Model hat predictd: {}\\n und Rictig ist: {}\\n\".format(predicted_label, real_label))\n",
    "    plt.imshow(image)\n",
    "label = 'Ergebnisse von TensorRT in {} secunde Zeit '.format(sum_zeit/len(images))\n",
    "fig.suptitle(label, fontsize=16, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
